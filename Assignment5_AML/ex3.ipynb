{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZTpgsmnWN36"
      },
      "source": [
        "In this part our goal is to reach the goal state G from the starting state S without visiting the hole states H. That is, while trying to reach the goal state G from the starting state S, if the agent visits the hole states H,then it will fall into the hole and die as Figure shows:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "• S implies the starting state\n",
        "\n",
        "• F implies the frozen states\n",
        "\n",
        "• H implies the hole states\n",
        "\n",
        "• G implies the goal state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027ZrUdlWN3-"
      },
      "source": [
        "## Solving the problem with value iteration\n",
        "In the previous part, we learned about the Frozen Lake environment. we want the agent to avoid the hole states H to reach the goal state G.\n",
        "How can we achieve this goal? That is, how can we reach state G from S without\n",
        "visiting H? We learned that the optimal policy tells the agent to perform the correct\n",
        "action in each state. So, if we find the optimal policy, then we can reach state G\n",
        "from S without visiting state H. Okay, how can we find the optimal policy? We\n",
        "can use the value iteration method we just learned to find the optimal policy.\n",
        "\n",
        "Remember that all our states (S to G) will be encoded from 0 to 16 and all four\n",
        "actions—left, down, up, right—will be encoded from 0 to 3 in the Gym toolkit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Hnf1vwWN3_",
        "outputId": "ed1661f3-9d48-4a38-ba50-83d9ea2895a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# First, let's import the necessary libraries:\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Now, let's create the Frozen Lake environment using Gym:\n",
        "env = gym.make('FrozenLake-v1')\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D2-8QsFWN4A"
      },
      "source": [
        "The preceding code will display\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "let's learn how to compute the optimal policy using the value\n",
        "iteration method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1tTA35AWN4B"
      },
      "source": [
        "In the value iteration method, we perform two steps:\n",
        "1. Compute the optimal value function by taking the maximum over the Q\n",
        "function, that is\n",
        "\n",
        "     ![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "2. Extract the optimal policy from the computed optimal value function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sviWGD56WN4B"
      },
      "source": [
        "## Computing the optimal value function\n",
        "We will develop a function named `value_iteration` to iteratively compute the optimal value function by maximizing the Q function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tWHKB76qWN4B"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "num_iterations = 1000\n",
        "Set the threshold number for checking the convergence of the value function:\n",
        "threshold = 1e-20\n",
        "set the discount factor to 1\n",
        "'''\n",
        "def one_step_lookahead(state, V, env, gamma=0.99):\n",
        "    A = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            A[action] += prob * (reward + gamma * V[next_state])\n",
        "    return A\n",
        "# def value_iteration(env):\n",
        "    \"Write your code for the calculation here.\"\n",
        "    # return value_table\n",
        "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
        "\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            A = one_step_lookahead(state, V, env, gamma)\n",
        "            best_action_value = np.max(A)\n",
        "            delta = max(delta, np.abs(best_action_value - V[state]))\n",
        "            V[state] = best_action_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6WyoTJWN4C"
      },
      "source": [
        "## Extracting the optimal policy from the optimal value function\n",
        "\n",
        "In the previous step, we computed the optimal value function. Now, we will extract the optimal policy from the computed optimal value function.\n",
        "\n",
        "we define a function called extract_policy, which takes value_table as a\n",
        "parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QA_TGIihWN4C"
      },
      "outputs": [],
      "source": [
        "def extract_policy(V, env, gamma=0.99):\n",
        "    \"\"\"\n",
        "    استخراج سیاست بهینه بر اساس تابع ارزش.\n",
        "\n",
        "    پارامترها:\n",
        "    V: تابع ارزش بهینه.\n",
        "    env: محیط.\n",
        "    gamma: ضریب تخفیف.\n",
        "\n",
        "    خروجی:\n",
        "    policy: سیاست بهینه.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    for state in range(env.observation_space.n):\n",
        "        A = one_step_lookahead(state, V, env, gamma)\n",
        "        best_action = np.argmax(A)\n",
        "        policy[state] = best_action\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD2xAnfIWN4C"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "We learned that in the Frozen Lake environment, our goal is to find the optimal\n",
        "policy that selects the correct action in each state so that we can reach state G from\n",
        "state A without visiting the hole states.\n",
        "First, we compute the optimal value function using our value_iteration function by\n",
        "passing our Frozen Lake environment as the parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AndQcF1fWN4D"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1')\n",
        "optimal_value_function = value_iteration(env)\n",
        "optimal_policy = extract_policy(optimal_value_function, env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9Co2l5WN4D"
      },
      "source": [
        "Next, we extract the optimal policy from the optimal value function using our\n",
        "extract_policy function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NElseVehWN4D",
        "outputId": "a9ad8979-3229-4d12-943f-b9b084ccb218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سیاست بهینه (هر عدد نمایانگر اقدام بهینه در هر حالت است):\n",
            "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "تابع ارزش بهینه:\n",
            "[0.54201404 0.49878743 0.47067727 0.45683193 0.5584404  0.\n",
            " 0.35834012 0.         0.59179013 0.64307363 0.61520214 0.\n",
            " 0.         0.74171617 0.86283528 0.        ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"سیاست بهینه (هر عدد نمایانگر اقدام بهینه در هر حالت است):\")\n",
        "print(optimal_policy)\n",
        "print(\"تابع ارزش بهینه:\")\n",
        "print(optimal_value_function)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}